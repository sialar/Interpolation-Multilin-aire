
\section{Introduction}\label{sec:1}
\hspace{0.5cm}
Un nouveau cadre pour les calculs neutroniques de base développé à EDF R\&D, dans
le département de SINETICS (SImulation NEutronique, Technologie de l'Information,
Calcul Scientifique) est basé sur la résolution de l'équation de Boltzmann
(ou une approximation) pour les neutrons.\\
Cette équation nécessite en entrée des sections efficaces qui modélisent les
interactions entre les neutrons induits par la fission et les noyaux provenant
soit du combustible soit du modérateur. \\

Ces sections efficaces (au nombre de plusieurs milliers) dépendent
de $d$ paramètres locaux (dits paramètres de rétroaction), tels que la densité de l'eau,
la concentration en bore, la température du carburant, le brûlage du combustible, etc.
Elles sont stockées dans des «bibliothèque nucléaire» et doivent être consultées
régulièrement tout au long de la simulation quand les paramètres de rétroaction évoluent. \\

Dans le cadre d’une thèse CIFRE \cite{These} qui vient d’être soutenue au
Laboratoire J.-L. Lions, une nouvelle méthode \cite{Luu} basée sur l’utilisation
de bases tensorielles a été développé. Ces méthodes utilisent des fonctions directionnelles
adaptées aux fonctions qu’on veut représenter et qui permet de diminuer le stockage, le coût
des calculs pour reconstruire les sections efficaces avec une très grande précision. \\

L'objectif du stage est de comparer cette étude avec une approche alternative (méthodes parcimonieuses)
proposée récemment au Laboratoire J.-L. Lions par Albert Cohen \cite{Albert}.
Le stage porte sur la mise en œuvre de ces méthodes parcimonieuses pour le cas particulier de l'approximation
des sections efficaces. Il s'agit pour certaines d'entre elles de méthodes non-intrusives (donc basées sur
des évaluations obtenues par un code qui peut être vu comme une boite noire,
ce qui est bien adapté à la situation). Elles font appel à des techniques de représentations parcimonieuses
dans le but de tirer parti des anisotropies potentielles dans la fonction qu'on veut capturer
(certaines variables pouvant en particulier être plus sensibles que d'autres) et des propriétés
de régularité en fonction des différentes variables.

\paragraph{Mots clés:} Sections efficaces, méthodes parcimonieuses, interpolation multivariée ...

%-------------------------------------------------------------------------------------------------------------%
\newpage
\section{Présentation de la structure d’accueil}\label{sec:2}
\hspace{0.5cm}
Le laboratoire, créé en 1969, porte le nom de son fondateur Jacques-Louis Lions.
Il s’agit maintenant d’une unité de recherche conjointe à l’Université Pierre et Marie Curie,
à l’université Paris Diderot et au Centre National de la Recherche Scientifique. \\

Le Laboratoire Jacques-Louis Lions constitue le plus grand laboratoire de France et l’un
des principaux au monde pour la formation et la recherche en mathématiques appliquées. \\

Il accueille l’activité de deux masters deuxième année ce qui représente une centaine d’étudiants.
Ses axes de recherche recouvrent l’analyse, la modélisation et le calcul scientifique haute
performance de phénomènes représentés par des équations aux dérivées partielles.
Fort d’environ 100 enseignants-chercheurs, chercheurs, ingénieurs, personnels administratifs
permanents ou émérites, et d’autant de doctorants ou post-doctorants, le LJLL collabore avec
le monde économique et avec d’autres domaines scientifiques à travers un large spectre
d’applications: dynamique des fluides; physique, mécanique et chimie théoriques; contrôle,
optimisation et finance; médecine et biologie; traitement du signal et des données.
\begin{center}
\includegraphics[height=8cm,width=0.9\linewidth]{images/Localisation.png}
\captionof{figure}{Localisation de la structure d'accueil}
\end{center}

%-------------------------------------------------------------------------------------------------------------%
\newpage
\section{Présentation de la problématique du projet}\label{sec:3}

\subsection{Présentation du contexte}\label{sec:3.1}
\hspace{0.5cm}
Afin d'exploiter au mieux son parc nucléaire, la R\&D d'EFD est en train de développer
une nouvelle chaîne de calcul appellée ANDROMEDE, pour simuler le cœur des réacteurs
nucléaires avec des outils à l'état de l'art. L'un des éléments de cette chaîne est le code
neutronique COCAGNE, dont l'un des objectifs est de résoudre numériquement l'équation du
\textbf{transport neutronique} (ou l'une de ses approximations), permettant ainsi d'obtenir des
grandeurs physiques d'intérêt pour décrire le comportement du réacteur, telle que:
le flux neutronique, le facteur de multiplication effectif, la réactivité ... \\
La résolution de cette équation nécessite une grande quantité de données physiques,
en particulier les \textbf{sections efficaces}. \\

En neutronique, les sections efficaces représentent la probabilité d'interaction d'un
neutron incident avec les noyaux cibles, pour différents types d'interaction. Dans une
simulation neutronique, les sections efficaces peuvent être représentées comme des fonctions
dépendant de plusieurs paramètres physiques. Ces paramètres sont utilisés pour décrire les
conditions thermo-hydroliques et la configuration du cœur du réacteur, tel que: la température
du combustible, concentration en bore, niveau de xénon, burnup, ... \\
Ainsi les sections efficaces sont des fonctions multivariées définies sur un espace appelé l'\textbf{espace de phase
des paramètres}. \\

Dans la simulation d'un cœur complet, le nombre de valeurs des sections efficaces est de l'ordre
de plusieurs milliards. Leur détermination demande un calcul complexe et lent.
Pour résoudre ce problème, un schéma en deux étapes est proposé afin de réduire la complexité
des calculs et d'accomplir la simulation du cœur. Ce schéma est basé sur la structure multi-échelle
du noyau du réacteur: noyau contenant des assemblages, assemblages contenant des tiges/cellules (voir figure 2).
\begin{center}
\includegraphics[height=6cm,width=10cm]{images/figure2.jpg}
\captionof{figure}{Schéma à 2 étapes basé sur la structure multi-échelle du noyau du réacteur}
\end{center}

\newpage
Par conséquent, 2 étapes sont effectuées séparément par deux codes:
\begin{itemize}
		\item \textbf{Code réseau:} permet de précalculer les sections efficaces sur des points
		présélectionnés dans l'espace de phase des paramètres (sur chaque assemblage). L'équation du transport neutronique est
		résolue de manière précise grâce à des discrétisations spatiales et énergétique (calcul hors ligne).
		L'information obtenue sur les sections efficaces est stockée dans des fichiers (appelés les \textbf{bibliothèques neutroniques}).
		\item \textbf{Code de cœur:} permet d'évaluer les sections efficaces en n'importe quel point du cœur par interpolation multivariée.
		Les informations stockées dans les biblothèques neutroniques sont utilisées comme données (points d'interpolations)
		pour résoudre l'équation du transport neutronique au niveau du cœur du réacteur.
\end{itemize}
Le schéma ci-dessous (figure 3) illustre le modèle de reconstruction des sections efficaces.
\begin{center}
\includegraphics[height=6cm,width=12cm]{images/figure1.png}
\captionof{figure}{Schéma à 2 étapes pour la simulation du cœur du réacteur}
\end{center}

\hspace{0.5cm}
Avec ce modèle, le nombre de points de discrétisation augmente exponentiellement en fonction
du nombre de paramètres ou de manière considérable quand on ajoute des points sur un des axes.
En effet, si on suppose que les sections efficaces dépendent de $d$ paramètres et que chaque
paramètre est discrétisé par $n$ points sur l'axe correspondant, dans ce cas le nombre
de calculs hors ligne et la taille des bibliothèques neutroniques sont de l'ordre de $O(n^d)$.

\newpage
\subsection{Objectifs attendus, problème à résoudre}\label{sec:3.2}
\hspace{0.5cm}
Avec les contraintes industrielles imposées à EDF, le modèle de l'interpolation multivariée
devient très coûteux en mémoire et en temps de calcul pour des situations complexes à cause
de l'augmentation rapide et exponentielle du nombre de points de calcul.
Ces situations peuvent arriver, par exemple, dans le cas accidentel où de nouveaux
paramètres s'ajoutent (température de l'eau, taille des lames d'eau) et/ou quand le domaine
de calcul s'étend (avec des domaines de définition plus larges). \\

Afin de dépasser les limitations de ce modèle, l'objectif de ce stage est de développer
un modèle de reconstitution des sections efficaces tout en répondant aux exigences suivantes:
\begin{itemize}
		\item \textbf{Calculs hors ligne:}
		\begin{itemize}
				\item utiliser moins de précalculs et ceci en choisissant les points de discrétisation de manière astucieuse.
				\item stocker moins de données pour la reconstruction des sections efficaces.
		\end{itemize}

		\item \textbf{Calculs en ligne:} 
		\begin{itemize}
				\item avoir une bonne précision pour l'évaluation des sections efficaces.
		\end{itemize}

\end{itemize}

%-------------------------------------------------------------------------------------------------------------%
\newpage
\section{Solution technique mise en œuvre}\label{sec:4}
\hspace{0.5cm}
Le but de cette partie est de présenter une méthode d'interpolation polynomiale adaptative en grande dimension.
Cette méthode permet d'approximer une fonction multivariée avec une forte précision tout en
utilisant moins de données. \\

Dans la section précédente, on a vu que les sections efficaces sont des fonctions multivariées
définies sur un espace appelé l'espace de phase des paramètres qu'on note $\mathcal{P}$. \\
On peut donc modéliser une section efficace par une fonction $f$ tel que: \\
$f:\mathcal{P} = P^d \rightarrow V_{\Lambda}$ avec $P$ un compact de $\mathbb{R}$ (ou $\mathbb{C}$) et
$V$ l'espace des valeurs prises par les sections efficaces. \\

En pratique, pour $y \in \mathcal{P}$, on peut approcher $f(y)$ avec un code très couteux. Le but est d'éviter ce lent
calcul, tout simplement, en considérant une approximation de $f$ en $y$, qu'on note $\tilde{f}(y)$.
Pour cela, il faut commencer par choisir un certain nombre de point $y^i \in \mathcal{P}, i \in 1..m$. Ensuite on
évalue $f_i = f(y^i) = f(y_1^i, .. , y_d^i)$ en faisant $m$ appel au code couteux. Ensuite, on utilise
$y^1,..,y^m,f_1,..,f_m$ pour fabriquer $\tilde{f}$ par interpolation.\\

L'objectif est donc de construire un opérateur d'interpolation $I_{\Lambda}$ qui permet de reconstruire une
fonction $f$ définie sur $\mathcal{P}$ à valeurs dans $V_{\Lambda}$. \\
Les méthodes d'interpolation polynomiale d'ordre supérieur construisent des approximations de la forme
$u_{\Lambda}(y) = \sum_{\nu \in \Lambda} u_{\nu} y^{\nu}$ avec $\Lambda \in \mathcal{F}$ un ensemble fini de multi-indices
$\nu = (\nu_j)_{j \geq 1} \in \mathcal{F}$ et $y^{\nu} = \prod_{j \geq 1} y_j ^ {\nu_j}$. \\

\textbf{Remarque:}
En dimension finie ($d < \infty$), l'ensemble d'indices $\mathcal{F}$ coincide avec $\mathbb{N}_0^d$.\\
Les $u_{\Lambda}$ sont choisis dans l'espace $V_{\Lambda}$, défini par $V_{\Lambda} = vect \left \{ \sum_{\nu \in \Lambda} v_{\nu} y^{\nu} : v_{\nu} \in V \right \}$

\subsection{Description de la solution envisagée}\label{sec:4.1}
\hspace{0.5cm}
Afin d'approximer $f$ en tout point, on va procéder par interpolation multivariée. Dans cette partie, on va revoir le principe de
l'interpolation de Lagrange en $1D$, puis en dimension quelconque. Par la suite, on évaluera le coût d'une telle opération et son évolution en fonction
du nombre de points d'interpolation et de la dimension de l'espace de phase des paramètres.
Ensuite, on présentera une meilleure méthode d'interpolation adaptative en grande dimension (moins couteuse) basée sur une construction hiérarchique de l'opérateur d'interpolation. \\

\subsubsection{Interpolation monovariée et tensorisation:}\label{sec:4.1.1}
\hspace{0.5cm}
\textbf{ - Cas $d=1$: }
Soit $(y_k)_{k \geq 0}$ une séquence de points deux à deux distincts. \\
On note $I_k$ l'opérateur d'interpolation polynomiale associé à
la séquence $\left \{ y_0, .. , y_k \right \}$. \\
Supposons que la fonction $f$ qu'on veut interpoler est à valeurs dans $\mathbb{R}$ (ou $\mathbb{C}$).\\
\newpage
On a alors,
\begin{align}
   I_k(f) & = \sum_{i=0}^k f(y_i) l_i^k
\end{align}
avec $l_i^k(y) = \prod_{j=0,\ i \neq j}^k \frac{(y - y_j)}{(y_i - y_j)}$, polynômes d'interpolation de Lagrange
de degrès $(k-1)$ associé à la séquence $\left \{ y_0, .. , y_k \right \}$ (voir figure 4). \\
\begin{center}
\includegraphics[height=6cm,width=10cm]{images/lagrange_polynomials.png}
\captionof{figure}{Exemples de polynômes d'interpolation de Lagrange}
\end{center}

\vspace{1cm}
\hspace{0.5cm}
\textbf{ - Cas $d>1$: }
On procède par tensorisation. \\
C'est à dire, pour $\alpha_1,..,\alpha_d \in \left \{0,.., k-1 \right \}$ \\
\begin{align}
   \tilde{f} (y_1,..,y_d)& = \bigotimes_{i=1}^d I_k (f)(y_1,..,y_d) \in \mathbb{Q}_{k-1} = vect \left \{ y \rightarrow y_1^{\alpha_1}..y_d^{\alpha_d} \right \} \\
	 & = \sum_{k_1=1}^k..\sum_{k_d=1}^k (f(y_{k_1},..,y_{k_d}) l_{k_1}(y_1)..l_{k_d}(y_d))
\end{align}
\hspace{0.5cm}
Pour avoir une bonne précision, il est préférable de choisir un nombre relativement grand de points d'interpolation.
Si par exemple, on prend $k_j$ points suivant la direction $j$, on obtient un nombre total de points égal à $\prod_{i=1}^d k_j$.
Ceci pose problème, bien évidemment, lorsque $d$ est grand. En effet, non seulement le calcul de $\tilde{f}$ sera couteux, mais aussi,
si on souhaite ajouter $1$ points suivant la variable $i$ ça revient à ajouter $\prod_{j=1,\\ j \neq i}^d k_j$ noeuds dans la grille (voir figure 5).\\
\begin{center}
\includegraphics[height=7cm,width=12cm]{images/grille.png}
\captionof{figure}{Influence de l'ajout d'un point d'interpolation sur une seule direction}
\end{center}

Cette méthode d'interpolation devient très coûteuse lorsqu'il s'agit de fonctions multivariées en grande dimension.\\
Une solution serait de calculer l'opérateur d'interpolation par une méthode hiérarchique. C'est à dire qu'on va
utiliser les approximations de $f$ à l'ordre $j<k$ ($I_{j}(f)$ obtenue avec $j$ points d'interpolation) pour approcher $I_{k}(f)$.

\subsubsection{Construction hiérarchique de l'opérateur d'interpolation:}\label{sec:4.1.2}
\hspace{0.5cm}
\textbf{ - Cas $d=1$: }
Soit $k \geq 0$, on définit l'opérateur différence $\Delta_k$ par:
\begin{align}
   \Delta_k & =  I_k - I_{k-1}
\end{align}
On suppose, par convention, que $I_{-1}$ est l'opérateur nul. De cette façon, $I_0 = \Delta_0$ correspond à
l'opérateur qui à $f$ associe le polynôme constant $f(y_0)$.\\
On a alors,
\begin{align}
   I_n & = \sum_{k=0}^n \Delta_k
\end{align}
On définit ensuite, les polynômes hiérarchiques de degré $k$ associés à la séquence $\left \{ y_0, .. , y_k \right \}$ par
\begin{align}
	 h_k(y) & = \prod_{j=0}^{k-1} \frac{(y - y_j)}{(y_k - y_j)}, k > 0,\ et\ h_0(y) = 1,
\end{align}
On a alors,
\begin{align}
	\Delta_k(f) & = \alpha_k(f)h_k, \ \ \ \alpha_k(f) = f(y_k) - I_{k-1} f(y_k)
\end{align}
On obtient alors, par un processus itératif, la représentation suivante:
\begin{align}
   I_n(f) & = \sum_{k=0}^n \alpha_k(f)h_k
\end{align}
Ainsi pour calculer l'interpolant de $f$ en utilisant $n$ points, il suffit de connaître les intérpolants de $f$ d'ordre $i$,
$i \in \left \{1,..,n-1 \right \}$, et ceci d'une manière relativement rapide. \\

\hspace{0.5cm}
\textbf{ - Cas $d>1$: }
On veut effectuer une interpolation polynomiale pour une séquence imbriquée d'ensemble $(\Lambda_n)_{n \geq 1}$ avec $n=\sharp(\Lambda_n)$. \\
Pour la suite, on définit :
\begin{itemize}
		\item Le point multivarié $\textbf{y}_{\nu}$ par $\textbf{y}_{\nu} = (y_{\nu_j})_{j \geq 1} \in \mathcal{P}$
		\item La fonction polynomiale hiérarchique tensorisée $\textbf{H}_{\nu}$ par $\textbf{H}_{\nu}(y) = \prod_{j \geq 1} h_{\nu_j} (y_j)$
\end{itemize}
		Pour que la méthode décrite précedament en $1D$ fonctionne en dimension $d>1$, il faut s'assurer que pour tout $n \geq 1,\ \Lambda_n$ est monotone. \\

\fbox{\begin{minipage}{0.9\textwidth}
\textbf{Définition: }Un ensemble $\Lambda \subset \mathcal{F}$, non vide, est dit monotone, si
\begin{align}
  	& \nu \in \Lambda \ et\ \mu < \nu \Rightarrow \mu \in \Lambda
\end{align}
\hspace{2cm} avec $\mu \leq \nu$ signifie que $\mu_j \leq \nu_j$ pour tout $j$. \\
\end{minipage}}

\vspace{0.5cm}
Dans cette configuration, $\Lambda_n$ peut être vu comme une section $\left \{ \nu^1, .. , \nu^n \right \}$ d'une séquence
$(\nu^k)_{k \geq 1} \in \Lambda^{\mathbb{N}}$. Cette observation mène à un algorithme efficace pour le calcul de
$I_{\Lambda_n}f$ à partir de $I_{\Lambda_{n-1}}f$ et de la valeur de $f$ en le nouveau point $\textbf{y}_{\nu^n}$. En effet,
on remarque que $\Delta_{\nu^n}$ est multiple de la fonction hiérarchique tensorisée $\textbf{H}_{\nu^n}$ défini précedement.\\
Puisque $\textbf{H}_{\nu^n}(\textbf{y}_{\nu^n}) = 1$, alors \\
\begin{align}
	 \Delta_{\nu^n}f &= \Delta_{\nu^n} f(\textbf{y}_{\nu^n}) \textbf{H}_{\nu^n} \\
	 &= (I_{\Lambda_n}f(\textbf{y}_{\nu^n}) - I_{\Lambda_{n-1}}f(\textbf{y}_{\nu^n})) \textbf{H}_{\nu^n} \\
	 &= (f(\textbf{y}_{\nu^n}) - I_{\Lambda_{n-1}}f(\textbf{y}_{\nu^n})) \textbf{H}_{\nu^n}
\end{align}
Donc
\begin{align}
	 I_{\Lambda_n}f & = I_{\Lambda_{n-1}}f + (f(\textbf{y}_{\nu^n}) - I_{\Lambda_{n-1}}f(\textbf{y}_{\nu^n})) \textbf{H}_{\nu^n}
\end{align}
Par conséquent, les polynômes $I_{\Lambda_n}f$ sont donnés par:
\begin{align}
	 I_{\Lambda_n}f & = \sum_{k=0}^n f_{\nu^k} \textbf{H}_{\nu^k}
\end{align}
avec $f_{\nu^k}$ définis récursivement par:
\begin{align}
	 f_{\nu^1} & = f(y_0),\ \ f_{\nu^{k+1}} = f(\textbf{y}_{\nu^{k+1}}) - I_{\Lambda_{k}}f(\textbf{y}_{\nu^{k+1}}) = f(\textbf{y}_{\nu^{k+1}}) - \sum_{i=1}^k f_{\nu^i} \textbf{H}_{\nu^i}(\textbf{y}_{\nu^{k+1}})
\end{align}

\newpage
\subsubsection{Interpolation adaptative et séquence de points d'interpolation:}

\hspace{0.5cm}
Les ensembles $\Lambda_n$ peuvent être choisis préalablement de sorte qu'ils forment une
séquence imbriquée d'ensembles monotones. Ils peuvent aussi être construits d'une manière astucieuse
lors du calcul de l'interpolant en construisant un chemin qui correspond à un ordre entre les points d'interpolations. \\

Soit l'analogie suivante: si $(\textbf{H}_{\nu})_{\nu \in \mathcal{F}}$ est une base orthoromée de $L^2(\mathcal{P})$ alors
le choix d'un ensemble d'indices $\Lambda_n$ qui minimise l'erreur  serait de prendre les $n$ plus grands $a_{\nu} \left | f_{\nu}  \right |$
tel que $a_{\nu} = \left \| \textbf{H}_{\nu} \right \|_{L^{\infty}(\mathcal{P})} = \prod_{j \geq 1} \left \| {h_{\nu_j}} \right \|_{L^{\infty}(\mathcal{P})}$. \\
Cette stratégie permet d'avoir une séquence imbriquée $(\Lambda_n)_{n \geq 1}$, cependant il n'est pas certain que les
ensembles $\Lambda_n$ soient monotones. \\
Afin de résoudre ce problème, on définit la notion de voisins pour n'importe quel ensemble monotone $\Lambda$ par, \\
\begin{align}
	 \mathcal{N}(\Lambda) & = \left \{ \nu \notin \Lambda : R_{\nu} \in \Lambda \cup \left \{ \nu \right \} \right \}, R_{\nu} = \left \{ \mu \in \Lambda : \mu < \nu \right \}
\end{align}
Ainsi, avec cette définition, l'algorithme ci-dessous mène à une séquence imbriquée d'ensembles monotones. \\
\vspace{0.5cm}

\textbf{Algorithme d'Interpolation Adaptative:}

\vspace{0.5cm}
\fbox{\begin{minipage}{0.9\textwidth}
\begin{itemize}
\item Commencer par $\Lambda_1 = \left \{0_{\Lambda} \right \}$
\item En supposant $\Lambda_{n-1}$ calculé, trouver $\nu^n = argmax \left \{ a_{\nu} \left | f_{\nu} \right | : \nu \in \mathcal{N}(\Lambda_{n-1}) \right \}$, et définir $\Lambda_n = \Lambda_{n-1} \cup \left \{ \nu \right \}$
\end{itemize}
\end{minipage}}

\vspace{0.5cm}

Il est important de noter que le choix de la séquence de points d'interpolation joue un rôle critique
pour la stabilité de l'opérateur d'interpolation multivariée définie par la constante de Lebesgue:
\begin{align}
		\mathbb{L}_{\Lambda} = sup_{f \in B(\mathcal{P})-{0}} \frac{\left \| I_{\Lambda}f \right \|_{L^{\infty}(\mathcal{P})}}{\left \|f \right \|_{L^{\infty}(\mathcal{P})}}
\end{align}
avec $B(\mathcal{P})$ est l'espace des fonction bornées définis sur $\mathcal{P}$\\
L'objectif est de choisir la séquence $(y_k)_{k \geq 0}$ tel que les constantes monovariées de Lebesgue
\begin{align}
		\lambda_k & = max_{f \in B(\mathcal{P})-{0}} \frac{\left \| I_k f \right \|_{L^{\infty}(\mathcal{P})}}{\left \|f \right \|_{L^{\infty}(\mathcal{P})}}
\end{align}
associées à l'opérateur d'interpolation monovariée $I_k$ croient modérément par rapport à k.
Une telle séquence peut être construite en fixant $y_0 \in P$ et en définissant inductivement
\begin{align}
		y_k & = argmax_{y \in P} \prod_{j=0}^{k-1} \left | y - y_j \right |
\end{align}
Cette séquence $(y_k)_{k \geq 0}$ est appellée séquence de Leja \cite{Leja} sur P. Elle modère la croissance des constantes de Lebesgue
et a une implication intéressante sur le choix adaptatif des ensembles $\Lambda_n$.

La précision de l'interpolation dépend fortement du choix des fonctions hiérarchiques de base et de la séquence des points d'interpolation.
En effet, quand il s'agit d'approcher des fonctions plates ou polynomiales de n'importe quel ordre, considérer les polynômes hiérarchiques de Lagrange ainsi que la séquence de Leja est un
choix optimal. En effet, non seulement, on obtient l'interpolant au bout d'un nombre d'itérations relativement petit, mais aussi avec une erreur d'interpolation nulle. \\
Cependant, quand il s'agit d'approcher des fonctions plus complexes, par exemple des fonctions présentant de fortes variations brusque, ou certaine irrégularité, le choix précedant n'est
plus intéressant.\\
On a vu que l'algorithme AI évolue en localisant les points où l'erreur d'interpolation est la plus élevé. Parfois, il est difficile, voir impossible, de détécter
un point où l'erreur est très grande. Pour mieux voir cela, considérons l'exemple de la figure suivante.
%%%%%%%%%%%%%%%%%%%
Tant que l'erreur n'est pas évalué au niveau d'un point de l'intervalle $x_c-\epsilon/2, x_c+\epsilon/2$, l'algorithme AI ne sera pas efficace.
Ceci est dû au choix des fonctions de bases. En effet, dans de telles situations il est plus judicieux d'utiliser des fonctions polynômes par morceaux comme fonctions hiérarchiques de base.\\
Dans ce cas, on choisit une nouvelle séquence de points d'interpolation mieux adéquate. Cette séquence sera formée par un ensemble de points obtenu en discrétisant pogréssivement le domaine par dichotomie.\\
Voici, un exemple pour mieux comprendre ce qui précede.
Supposons que P = [-1,1], et que le premier point d'interpolation est 0. Alors, la séquence de point d'interpolation évoluera de la façcon suivante.
t = 0, s = {0}, v = {-1,1}
t = 1, s = {0,-1}, v = {1,-0.5} ou s = {0,1} v = {-1,0.5}
t = 2, s = {0,-1,1} v = {-0.5,0.5} ou s = {0,-1,-0.5} v = {1.-0.25,-0.75} ou s = {0,1,-1} v = {-0.5,0.5} ou s = {0,1,0.5} v = {-1.0.25,0.75}
Ainsi de suite, jusqu'à obtenir un arbre de points de la forme:

Etant donné qu'on a besoin d'une notion d'ordre entre les points d'interpolation, on introduit la relation suivante:
Un noeud n est d'ordre plus grand qu'un noeud m si et seulement si m est un ascendant de n. Avec cette supposition, certains points sont incomparable (exemple noeud n_0 et n_1).
Maintenant, en grande dimension, et avec ce type de séquence, on est capable de définir un ensemble monotone vu qu'on a une relation d'ordre entre les noeuds.


\subsection{Implèmentation de la solution}\label{sec:5}
Dans cette partie, on va mettre en pratique cette méthode et on va présenter certains détails
de l'implémentation de cette solution (notamment la construction de la séquence de Leja).\\

Tout d'abord cette solution a été implémenté en C++. Elle a été, par la suite, téstée sur
des fonctions définies à l'avance en des points choisis d'une manière aléatoire.
Le code est constitué principalement de trois classes:
\begin{itemize}
		\item \textbf{Classe Interpolation:} elle contient les structures de données contenants les différentes quantités qui entrent
		en jeu dans la construction de l'opérateur d'interpolation, notamment les points d'interpolation, les points de test,
		les $f_{\nu}$ et le chemin d'indices vus dans la section précédente. De plus, l'algorithme AI ainsi que certaines fonctions intermédiaires,
		(notamment celle responsable de la recherche des voisins dans une grille) sont implémentés dans la classe $Interpolation$
		\item \textbf{Classe MultivaraintPoint:} il s'agit d'une classe générique qui modélise à la fois les multi-indices ainsi que les points réels multivariés.
		C'est à dire qu'une instance de cette classe peut renfermer soit des valeurs réelles ou alors entières (multi-indice). Les opérateurs d'affectation,
		d'addition, d'affichage ainsi que de comparaison de points multivariés sont implémentés dans cette classe.
		\item \textbf{Classe Utils:} il s'agit d'une classe statique renfermant certaines fonctions utiles notamment les fonctions d'affichages,
		les fonctions qui gèrent la création des séquences d'interpolations (notamment la séquence uniforme, les zéros de Tchebychev ainsi que la séquence de Leja).
\end{itemize}

Le choix de la séquence d'interpolation joue un rôle important dans la stabilité de l'opérateur d'interpolation. Dans la suite, on verra quelques détails
sur la construction de la séquence des points de Leja.
\paragraph{Séquence de Leja:}
L'idée est de discrétiser l'intervalle de définition $U$ de la fonction $f$ en un grand nombre de points $(a_i)_{i=1..n(=100000)}$.
Supposons qu'on a construit à l'étape $i$ la séquence $\left \{ y_0,..,y_i \right \}$, alors pour avoir le point $y_{i+1}$,
on compare le produit des distances de chaque point de discrétisation aux points de Leja déja calculés, puis on choisit le max.\\
La façon la plus naturelle de choisir les points $a_i$ est de les choisir uniformément répartis dans l'intervalle de définition de $f$.\\
On obtient alors une suite de polynômes qui interpole $f$ en de plus en plus de points. On pourrait s'attendre à ce que la suite converge
uniformément vers $f$ lorsque le nombre de points d'interpolation augmente.\\
Malheureusement, ce n'est pas le cas, ce phénomène est connu sous le nom de phénomène de Runge. \\
Une solution est, étonnamment, de ne pas choisir les points uniformément répartis. Une raison pour cela est l'inégalité suivante :
si $f$ est de classe $C^N$ sur l'intervalle $U$ et si $L$ est le polynôme d'interpolation de Lagrange en les points $a_1,..,a_N$, alors on a
\begin{align}
		& \forall x \in U, \left |f(x)-L(x)\right | \leq sup_{x \in U}  (\prod_{i=1}^n \left | x-a_i \right |) \frac{\left \|f^{(n)} \right \|_{\infty}}{n!}
\end{align}
\hspace{0.5cm}
L'idée est donc de choisir les $a_i$ de sorte que $sup_{x \in U}  \prod_{i=1}^n \left | x-a_i \right |$ soit le plus petit possible. On démontre que
ceci est réalisé lorsque les $a_i$ sont les zéros du polynôme de Tchebychev de degré $n$, à savoir $a_i = cos(\frac{(2i-1) \pi}{n}), i =1,..,n$.\\
La figure 6 montre une grille 2D formés par des points de Leja construits dans le segment $\left [-1,1 \right ]$.\\
\begin{center}
\includegraphics[height= 9 cm,width = \linewidth]{images/leja_sequence.png}
\captionof{figure}{Grille 2D des points de Leja}
\end{center}


\section{Évaluation de l’efficacité de la méthode}\label{sec:6}
\subsection{Résultats expérimentaux:}\label{sec:6.1}
Afin d'évaluer les performances de l'algorithme d'interpolation adaptative multivariée, des tests ont été faits sur
différentes fonctions, séquences d'interpolation et en plusieurs dimensions. Les résultats en matière de temps de calcul
et d'erreur d'interpolation sont résumés dans le tableau ci-dessous. La fonction interpolée dans ce test est la fonction $f$ définie par:
\begin{align}
		f :  \left [-1,1 \right ]^d & \rightarrow \mathbb{R} \\
		 x = (x_1,..,x_d) & \rightarrow sin(\left \| x \right \|^2) = sin(\sum_{i=1}^d x_i^2)
\end{align}

\begin{center}
\begin{tabular}{|*{3}{c|}}
  	\hline
		Nombre de points 			& 		Erreur d'interpolation 		 & 			Temps de calcul	(s) \\
		d'interpolation (d=2) &		$M_0$			 |    $M_1$        &     $M_0$	  |    $M_1$	\\
		\hline
		100 									& 1.5 $10^{-4}$ | 1.5 $10^{-4}$  &     0.172		| 	0.003		\\
		500 									& 8.0 $10^{-4}$ | 8.7 $10^{-5}$  &     21.24		| 	0.036		\\
		1000 									& 1.2 $10^{-6}$ | 2.4 $10^{-5}$  &     173.2		| 	0.157		\\
		5000 									& 3.2 $10^{-7}$	| 2.4 $10^{-7}$  &     $> 1h$   |		6.529		\\
		10000 								& 	 ---------	| 5.3 $10^{-7}$  &    $+ \infty$| 	32.21	  \\
		50000 								& 	 ---------  | 1.0 $10^{-8}$  &   	$+ \infty$| 	2270.	  \\
	  \hline
\end{tabular}
\end{center}

\vspace{0.5cm}
La méthode $M_0$ correspond à une première version de l'algorithme $AI$. Une fois téstée et validée, on est passé
à une version $M_1$ apportant certaines améliorations.
Les critères de validation sont les suivants:
\begin{itemize}
		\item L'interpolation donne un résultat exact (l'erreur est nulle), si la fonction interpolée est de nature
		polynomiale
		\item Le résultat d'interpolation ne dépend pas du chemin suivi. C'est-à-dire que si on fixe au préalable un ordre différent pour les points
		d'interpolation que celui obtenu par l'algorithme $AI$ (tout en gardant le caractère monotone de l'ensemble d'indices), on obtient le même résultat.
		\item Les coefficients $\alpha_{\nu}$ ou $f_{\nu}$  ne dépendent pas du chemin suivi. Ils ne dépendent que de la fonction interpolée et de la séquence d'interpolation.
\end{itemize}

Le tableau ci-dessous montre les résultats de l'interpolation de $f$ pour différentes valeurs de d.
Les temps de calcul présentés dans le tableau correspondent au temps que met l'algorithme $AI$
pour approximer $f$ avec une erreur $e \leq seuil=10^{-4}$. \\
On note $t$ le temps de calcul, et $n\_iter$ le nombre d'itérations efféctuées par l'algorithme avant de s'arrêter.\\

\textbf{Remarque:} Le nombre maximal d'itérations correspond au nombre total de points d'interpolation construit initialement. L'algorithme AI
sélectionne des points à partir de cette séquence initiale. Au cours de son exécution, chaque fois que l'algorithme effectue $10\%$ du nombre maximal d'itérations, il évalue l'erreur d'interpolation.
Si elle dépasse un certain seuil, l'algorithme s'arrête.
\begin{center}
\begin{tabular}{|*{4}{c|}}
  	\hline
		Nombre de points & 			d=2			 		&     d=3      & 		d=4     	\\
		d'interpolation  &	n\_iter | t   	&	 n\_iter | t &  n\_iter | t \\
		\hline
		1000 						 &   600 | 0.06			&	 900 | 0.16  &  1000 | 0.24 \\
		5000 						 &  1000 | 0.22     & 1500 | 0.46  &  3000 | 2.24 \\
		10000 					 &  1000 | 0.24     & 4000 | 4.83  &  7000 | 13.04	\\
		20000						 &  4000 | 5.47     & 8000 | 19.53 & 12000 | 40.92 \\
		\hline
\end{tabular}
\end{center}

\subsection{Optimisations apportées:}\label{sec:6.2}
\begin{itemize}
		\item \textbf{Recherche du voisin courant:} Dans $M_0$, on recherchait le nouveau voisin parmi tous les points de la grille.
 		Ce n'est pas évidemment la meilleure idée car la grille contient non seulement les voisins potentiels mais aussi les points déjà sélectionnés et traités.
		Donc, il est inutile de la parcourir en totalité à chaque étape car ça devient très couteux. \\
		Dans la version $M_1$, une meilleure stratégie a été adopté. En effet, on ne cherche plus le nouveau meilleur voisin parmi tous les points de la grille
		($~100$ si on est en dimension 2 et qu'on a choisi 10 points de discrétisation sur chaque direction), mais plutôt parmi un petit ensemble de points
		correspondant aux points voisins. (voir figure 7: à gauche la version $M_0$ et à droite, la version $M_1$. La recherche du nouveau voisin s'effectue dans l'ensemble
		délimité par un coutour bleu).
\end{itemize}
\begin{center}
\includegraphics[height= 9 cm,width = \linewidth]{images/algo_ai_recherch_voisin.png}
\captionof{figure}{Recherche du voisin courant}
\end{center}
\begin{itemize}
		\item \textbf{Calcul des $\alpha_{\nu}$ (ou $f_{\nu}$):} Dans $M_0$, à chaque étape de l'algorithme AI, on construit un sous-ensemble de points
		qui correspondent à tous les voisins courants. Ensuite on calcule tous les $\alpha_{\nu}$ ($\nu$ étant les multi-indices correspondants aux voisins courants)
		et on choisi le voisin ayant le plus grand $\alpha_{\nu}$. Il est possible qu'au cours d'une certaine itération (voir toutes les itérations)
		de recalculer un $\alpha_{\nu}$ déja calculé à l'itération précédente. Ceci est, bien évidemment, couteux. \\
		Dans $M_1$ on procède d'une manière plus astucieuse. En effet, on peut éviter cette redondance, tout simplement, en utilisant une structure de données
		qui donne un coût de recherche en $O(log(n))$. Par exemple, on peut utiliser le conteneur map de la STL. Dans ce cas, dès qu'on calcule un $\alpha_{\nu}$ pour
		un $\nu$ donné on ajoute à la map le couple $(\nu, \alpha_{\nu})$. Ainsi, si on a besoin, dans une itération ultérieure, de calculer ce même $\alpha_{\nu}$,
		on irait le chercher directement dans cette map avec un coût en temps logarithmique.
\end{itemize}

%-------------------------------------------------------------------------------------------------------------%
\section{Impressions personnelles}\label{sec:7}
\hspace{0.5cm}
Au début du stage et notamment au cours des premières présentations du sujet, ce dernier m'a paru
plutôt difficile. Il m'a fallu une certaine période de documentation et de recherche avant de pouvoir entamer la partie pratique.
Ainsi, au bout des premières semaines de mon stage, j'ai réussi à bien assimiler la partie théorique du sujet.
Ce sujet m'a paru alors encore plus intéressant car j'ai pu constater sa richesse en matière de spectre d'applications. De plus j'ai eu l'occasion
de beaucoup apprendre notamment sur l'interpolation en grande dimension en général, mais aussi au niveau de la programmation informatique. En effet, j'ai
pu améliorer mes connaissances en python et mettre en application tout ce que j'ai appris en algorithmique et en C++.\\

De plus, étant donnée que mes encadrants ont participé à l'étude et la réalisation de la partie théorique, il m'était facile d'évoluer
et d'avancer dans ma compréhension des différents points-clés de mon sujet de stage. En effet, le fait qu'ils soient souvent disponibles
pour discuter des points potentiellement imprécis et de l'organisation du travail m'a évité une importante perte de temps.\\
Par ailleurs, j'ai été agréablement surpris par l'environnement et l'esprit de travail au sein d'un laboratoire de recherche.\\
J'ai, de plus, apprécié la liberté qu'on m'a laissée pour le choix de l'environnement de travail et l'organisation des différentes taches et l'ordre de leurs réalisations.
Ceci m'a permit non seulement d'apprendre à être autonome mais aussi d'améliorer mes compétences en analyse et en programmation.

%-------------------------------------------------------------------------------------------------------------%
\section{Prise en compte de l’impact environnemental et sociétal}\label{sec:8}
\hspace{0.5cm}
Dans un monde de plus en plus impacté par l’évolution humaine, l’écologie est un sujet
souvent pris en compte de nos jours. On en parle beaucoup dans les domaines de transport, de la nutrition, de l'économie et du marketing, ...\\
Qu’en est-il alors des nouvelles technologies et de la recherche mathématique?\\

Actuellement étudiant à l’Ensimag et stagiaire dans un grand laboratoire de mathématiques appliquées,
ce sujet m'a intéressé et a aussi suscité l’attention des membres de mon équipe qui m’ont aidé à enquêter là-dessus\\
Je travaille quotidiennement avec 4 personnes dans un bureau éclairé au néon.
Nous utilisons chacun un ordinateur de bureau pendant 7h par jours.\\
Actuellement, le laboratoire met, à la disposition des employés, de plus en plus de moyens écologiques.
Par exemple, l’éclairage s’éteint automatiquement en absence prolongée de mouvement dans la piéce, de plus les déchets papiers sont recyclés.\\
Quant au sujet de mon stage, les méthodes mathématique qui y sont proposées permettent de réduire considérablement le coût des calculs des sections efficaces en neutronique,
à précision égale, et par conséquent de limiter la consommation énergétique de gros serveurs de calcul.
%-------------------------------------------------------------------------------------------------------------%

\section{Conclusion}\label{sec:9}

\newpage


\bibliographystyle{plain}
\bibliography{biblio}
